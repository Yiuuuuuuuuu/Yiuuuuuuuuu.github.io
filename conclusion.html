<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Future Development & Conclusion | Computational Linguistics</title>
    <link rel="stylesheet" href="styles/style.css">
</head>
<body>
    <header>
        <h1>CS1102 - Computational Linguistics: Future Development & Conclusion</h1>
        <div class="group-info">
            <p>Group Members: Luk Cheuk Yiu 57881645, Choi Cheuk Yin 57274102, Wong Tsz Mei 57186243, Shin Chun Lung 58020740</p>
            <p>Techniques Demonstrated: HTML / CSS / Javascript</p>
        </div>
    </header>

    <nav class="dropdown">
        <button class="dropbtn">Menu</button>
        <div class="dropdown-content">
          <a href="index.html">Home</a>
          <a href="advantages.html">Advantage & Challenge</a>
          <a href="transformation.html">Comparison & Transformation</a>
          <a href="conclusion.html">Future Development & Conclusion</a>
          <a href="references.html">References</a>
        </div>
    </nav>

    <main>
        <section>
            <h2>Conclusion & Future Directions</h2>
            
            <section>
              <h3>Trade-offs</h3>
              <ul>
                <li><strong>Traditional:</strong> Interpretable, resource-light, but rigid.</li>
                <li><strong>LLMs:</strong> Context-aware, high-performing, but computationally intensive.</li>
              </ul>
            </section>
            
            <section>
              <h3>Future Work</h3>
              <ul>
                <li>Hybrid models (e.g., integrating Four-Corner features into embeddings).</li>
                <li>Leveraging frameworks like Hugging Face for accessible, ethical LLM deployment.</li>
              </ul>
            </section>
            
            <section>
              <h3>Visual Appendix (Suggested Diagrams)</h3>
              
              <figure>
                <img src="bow-vs-embeddings.png" alt="BoW's sparse matrix vs embeddings' dense vectors">
                <figcaption>BoW's sparse matrix vs. embeddings' dense vectors preserving semantic relationships.</figcaption>
              </figure>
              
              <figure>
                <img src="four-corner-code.png" alt="Four-Corner encoding example">
                <figcaption>Traditional stroke-based encoding for æ²¹ (oil) vs. BERT's contextual vector.</figcaption>
              </figure>
              
              <figure>
                <img src="transformer-architecture.png" alt="Transformer architecture comparison">
                <figcaption>Multi-head attention in BERT (encoder) vs. GPT's autoregressive decoder.</figcaption>
              </figure>
            </section>
            
            <footer>
              <p>This version maintains your original examples (e.g., Chinese characters, QA tasks) while strengthening the comparative narrative.</p>
            </footer>
          </section>
    </main>
</body>
</html>