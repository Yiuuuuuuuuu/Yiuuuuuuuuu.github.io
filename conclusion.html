<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Future Development & Conclusion | Computational Linguistics</title>
    <link rel="stylesheet" href="styles/style.css">
</head>
<body>
    <header>
        <h1>CS1102 - Computational Linguistics: Future Development & Conclusion</h1>
        <div class="group-info">
            <p>Group Members: Luk Cheuk Yiu 57881645, Choi Cheuk Yin 57274102, Wong Tsz Mei 57186243, Shin Chun Lung 58020740</p>
            <p>Techniques Demonstrated: HTML / CSS / Javascript</p>
        </div>
    </header>

    <nav class="dropdown">
        <button class="dropbtn">Menu</button>
        <div class="dropdown-content">
          <a href="index.html">Home</a>
          <a href="advantages.html">Advantage & Challenge</a>
          <a href="transformation.html">Comparison & Transformation</a>
          <a href="conclusion.html">Future Development & Conclusion</a>
          <a href="references.html">References</a>
        </div>
    </nav>

    <main>
        <section>
            <h2>Future Development</h2>
            <p>Future work could focus on developing hybrid models
            that combine traditional techniques, such as Four-
            Corner features, with modern embeddings to achieve
            both interpretability and performance. Additionally,
            frameworks like Hugging Face provide opportunities
            for accessible and ethical deployment of LLMs,
            ensuring their adaptability to diverse datasets and
            applications.</p>
            <p>The visual appendix includes diagrams to clarify key
            comparisons and concepts. First, a diagram contrasts
            Bag of Words (BoW) and embeddings, showcasing
            the difference between BoW's sparse matrices and
            embeddings' dense vectors, which better capture
            semantic relationships.</p>
            <p>Another visual compares the traditional Four-Corner
            encoding for Chinese characters, such as æ²¹ (oil), with
            BERT's contextual embeddings, emphasizing the shift
            from rule-based to context-aware methods. Finally, a
            diagram illustrates the architectural differences
            between BERT's multi-head attention mechanism and
            GPT's autoregressive decoder, highlighting their
            unique approaches to processing and generating
            language.</p>
            
            <h2>Conclusion</h2>
            <p>This study highlights the trade-offs between
            traditional methods and large language models
            (LLMs). Traditional approaches are interpretable and
            resource-efficient but rigid in handling complex tasks.
            In contrast, LLMs offer superior context-awareness
            and performance but require significant computational
            resources, making them less accessible in some
            scenarios. Striking a balance between these
            approaches is a key challenge in advancing natural
            language processing and sentiment analysis.</p>
            
            
        </section>
    </main>
    <footer>
      <p>&copy; 2024 CS1102 Group Project - Computational Linguistics</p>
    </footer>
</body>
</html>