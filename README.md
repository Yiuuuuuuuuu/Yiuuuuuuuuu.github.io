<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CS1102 Project | Computational Linguistics</title>
    <link rel="stylesheet" href="styles/style.css">
</head>
<body>
    <!-- Header with Topic Name -->
    <header>
        <h1>CS1102 - Computational Linguistics: Traditional Methods to LLMs - 2024/2025 Semester B</h1>
        <div class="group-info">
            <p>Group Members: Luk Cheuk Yiu 57881645, Choi Cheuk Yin 57274102, Wong Tsz Mei 57186243, Shin Chun Lung 58020740</p>
            <p>Techniques Demonstrated: HTML / CSS / Javascript</p>
        </div>
    </header>

    <!-- Navigation Menu -->
    <nav class="dropdown">
        <button class="dropbtn">Main Menu</button>
        <div class="dropdown-content">
          <a href="index.html">Home</a>
          <a href="advantages.html">Advantage & Challenge</a>
          <a href="transformation.html">Transformation with future development</a>
          <a href="references.html">References</a>
        </div>
    </nav>

    <!-- Main Content -->
    <main>
        <section class="intro">
            <h2>Introduction</h2>
            <p>Computational linguistics has evolved from rule-based systems and statistical models to transformer-driven LLMs like BERT and GPT. This project compares these paradigms through tasks such as semantic analysis, chatbots, and text reasoning, highlighting how innovations in embeddings and attention mechanisms address longstanding limitations in processing diverse languages, including Chinese.</p>
        </section>

        <section class="techniques">
            <h2>What is LLMs?</h2>
            <p>Large Language Models (LLMs) are advanced artificial intelligence systems designed to understand, generate, and manipulate human language. They represent a significant leap forward in natural language processing capabilities.</p>
            <ul>
                <li><strong>Transformer Architecture:</strong> The foundation of modern LLMs</li>
                <li><strong>Tokenization:</strong> Process of converting text into numerical representations</li>
                <li><strong>Attention Mechanisms:</strong> Allow models to focus on relevant parts of text</li>
                <li><strong>Pre-training:</strong> Models learn from vast amounts of text data</li>
            </ul>
        </section>
    </main>

    <!-- Help Button -->
    <button id="guidelinesBtn">Q&A System Guidelines</button>

    <!-- QA -->
    <div class="container">
        <h1>Q&A System</h1>
        
        <label for="questionSelect">Select a question:</label>
        <select id="questionSelect">
            <option value="">-- Choose a question --</option>
            <option value="1">1. What is the role of tokenization in LLM processing?</option>
            <option value="2">2. Why are large language models better for chatbots?</option>
            <option value="3">3. What is the main limitation of transitional methods in computational linguistics?</option>
            <option value="4">4. What is word embedding?</option>
            <option value="5">5. What is the Attention Mechanism?</option>
        </select>
        
        <button id="submitBtn">Submit</button>
        
        <div id="answer"></div>
        
        <button id="translateToTraditionalBtn" disabled>Translate to Traditional Chinese</button>
        <button id="translateToSimpleBtn" disabled>Translate to Simple Chinese</button>
        
        <div id="translation"></div>
        
        <div id="sentiment">
            <h3>Sentiment Analysis</h3>
            <p id="sentimentResult">No answer submitted yet.</p>
        </div>
    </div>




    <script src="scripts/script.js"></script>
</body>
</html>